# Requirements Fine-Tune Light

This repository contains a Colab notebook for fine-tuning language models on requirements datasets.  
The goal is to experiment with lightweight architectures and training strategies for transforming raw requirements into structured forms, such as the EARS (Easy Approach to Requirements Syntax) template.

---

## 🚀 Features

- Load and preprocess requirements datasets
- Fine-tune transformer-based models (e.g., GPT-2, T5, BART)
- Evaluate model performance (BLEU, accuracy, etc.)
- Export predictions and logs for further analysis
- Designed for use in Google Colab, but can also run in local Jupyter environments

---

## 📓 Usage

1. **Open in Colab:**  
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huzaifa003/requirements-fine-tune/blob/main/Requirements_fine_tune_light_clean.ipynb)

2. **Run the Notebook:**  
   - Step through each cell, following the instructions and comments.
   - Modify dataset paths or hyperparameters as needed.

3. **Results:**  
   - Outputs, predictions, and logs are saved in the notebook or exported as files.

---

## 🛠️ Requirements

- Python 3.8+
- HuggingFace Transformers
- Datasets (or custom dataset CSV/JSON)
- Jupyter or Google Colab

> All necessary libraries can be installed using pip. See the first code cell in the notebook for installation instructions.

---

## 📝 About

This notebook is intended for research and experimentation on requirements engineering using NLP.  
Feel free to fork and adapt for your own use cases!

---

## 📬 Contributions & Issues

- Pull requests are welcome!
- For suggestions or bug reports, open an issue on GitHub.

---

## License

[MIT](LICENSE)

